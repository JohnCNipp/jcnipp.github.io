{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project3CPSC483.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOOo64D90hX3R9zjg28iZ4u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["John Nipp, Self-study, 6-15-2022\n","\n","The first thing we are going to do, is our basic import statements and setup. Make sure to upload the bank-additional-full.csv within the sample_data directory.\n","\n","Project Link here:\n","https://docs.google.com/document/d/17d5zMHkYPojlAQB9qA7PZGNiofkSV_ekPjQmkTQ8KmE/edit#"],"metadata":{"id":"ct5L3AL4Enjl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHR8wAAt2RBV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655864536691,"user_tz":420,"elapsed":396,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"ae0ab6b3-dc62-4723-b3cf-56a5d674d739"},"outputs":[{"output_type":"stream","name":"stdout","text":["               age      campaign      previous         pdays  emp.var.rate  \\\n","count  41188.00000  41188.000000  41188.000000  41188.000000  41188.000000   \n","mean      40.02406      2.567593      0.172963    962.475454      0.081886   \n","std       10.42125      2.770014      0.494901    186.910907      1.570960   \n","min       17.00000      1.000000      0.000000      0.000000     -3.400000   \n","25%       32.00000      1.000000      0.000000    999.000000     -1.800000   \n","50%       38.00000      2.000000      0.000000    999.000000      1.100000   \n","75%       47.00000      3.000000      0.000000    999.000000      1.400000   \n","max       98.00000     56.000000      7.000000    999.000000      1.400000   \n","\n","       cons.price.idx  cons.conf.idx     euribor3m   nr.employed  \n","count    41188.000000   41188.000000  41188.000000  41188.000000  \n","mean        93.575664     -40.502600      3.621291   5167.035911  \n","std          0.578840       4.628198      1.734447     72.251528  \n","min         92.201000     -50.800000      0.634000   4963.600000  \n","25%         93.075000     -42.700000      1.344000   5099.100000  \n","50%         93.749000     -41.800000      4.857000   5191.000000  \n","75%         93.994000     -36.400000      4.961000   5228.100000  \n","max         94.767000     -26.900000      5.045000   5228.100000  \n"]}],"source":["import numpy as np\n","import pandas as pd\n","import sklearn\n","\n","# Make it clear what we are take from the data\n","ze_dir = 'sample_data/bank-additional-full.csv'\n","\n","# Importing the data in a way that I'm familiar with\n","raw_data = pd.read_csv(ze_dir, delimiter = ';')\n","raw_data.head()\n","\n","# Printing summaries to give a first impression of the data\n","print(raw_data[['age', 'campaign', 'previous', 'pdays', 'emp.var.rate', \n","                'cons.price.idx', 'cons.conf.idx', 'euribor3m', \n","                'nr.employed']].describe())"]},{"cell_type":"markdown","source":["Now, we need to separate the data between our training set, and our test set. It shouldn't very hard, we'll use a well-known method from sklearn to split it between test and training data.\n","\n","I also went ahead and checked to see if we have any minimums we have to readjust for our naivebayes classifier. I will recenter those numeric columns to be at a minimum of 0 by \"subtracting\" the minimum value from the entire column.\n","\n","The project doesn't reccommend to separate the y values until after we stratify the data, but common practice is that we drop it before, this way we don't have to do similar code twice.\n","\n","We also need to get rid of duration, because the description says it won't help with creating a \"realistic prediction model\"."],"metadata":{"id":"QTqwCzfkkYSu"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Dropping duration\n","raw_data = raw_data.drop('duration', axis = 1)\n","\n","# Making the minimum of these two columns to be 0.\n","raw_data['cons.conf.idx'] -= min(raw_data['cons.conf.idx'])\n","raw_data['emp.var.rate'] -= min(raw_data['emp.var.rate'])\n","\n","# Saving a copy for later in the project\n","raw_data_copy = raw_data.copy()\n","\n","# Stratifying the y (dependant) variable\n","y_val = raw_data['y']\n","\n","# Dropping the dependant variable from the rest of the data\n","raw_data = raw_data.drop('y', axis = 1)\n","\n","# Separating the data between train data and test data\n","train_data, test_data, y_train, y_test = train_test_split( raw_data ,\n","                              y_val, test_size = .1, random_state=(2021-10-25))"],"metadata":{"id":"CzdCC38FmCqR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are going to be using algorithms that have problems with collinear features, so we will drop_first. \n","\n","We are going to have a hot-one-encoding type of data frame, where there are a lot of columns, each with a 1 or 0. We can do this, especially since our data is largely already clean."],"metadata":{"id":"2J4P2VtYS07-"}},{"cell_type":"code","source":["# Applying the encoding. Later, I will encode the data before stratifying\n","# the samples.\n","no_dummies_train = pd.get_dummies(train_data, drop_first=True)\n","no_dummies_test = pd.get_dummies(test_data, drop_first=True)\n","\n","# Verifying my output is correct\n","no_dummies_train.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"tN7IS5D-T2zs","executionInfo":{"status":"ok","timestamp":1655864536863,"user_tz":420,"elapsed":175,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"b56222d7-1d5d-4333-bd21-cfd45a3e1cf3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       age  campaign  pdays  previous  emp.var.rate  cons.price.idx  \\\n","22975   56         5    999         0           4.8          93.444   \n","14746   26         1    999         0           4.8          93.918   \n","12505   38         1    999         0           4.8          93.918   \n","6801    33         2    999         0           4.5          93.994   \n","18389   29         3    999         0           4.8          93.918   \n","\n","       cons.conf.idx  euribor3m  nr.employed  job_blue-collar  ...  month_may  \\\n","22975           14.7      4.965       5228.1                0  ...          0   \n","14746            8.1      4.957       5228.1                0  ...          0   \n","12505            8.1      4.960       5228.1                0  ...          0   \n","6801            14.4      4.857       5191.0                0  ...          1   \n","18389            8.1      4.968       5228.1                0  ...          0   \n","\n","       month_nov  month_oct  month_sep  day_of_week_mon  day_of_week_thu  \\\n","22975          0          0          0                1                0   \n","14746          0          0          0                0                0   \n","12505          0          0          0                1                0   \n","6801           0          0          0                0                0   \n","18389          0          0          0                0                1   \n","\n","       day_of_week_tue  day_of_week_wed  poutcome_nonexistent  \\\n","22975                0                0                     1   \n","14746                0                1                     1   \n","12505                0                0                     1   \n","6801                 0                1                     1   \n","18389                0                0                     1   \n","\n","       poutcome_success  \n","22975                 0  \n","14746                 0  \n","12505                 0  \n","6801                  0  \n","18389                 0  \n","\n","[5 rows x 52 columns]"],"text/html":["\n","  <div id=\"df-d5cb3020-9745-411c-ad50-9c6a7046ffb5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>campaign</th>\n","      <th>pdays</th>\n","      <th>previous</th>\n","      <th>emp.var.rate</th>\n","      <th>cons.price.idx</th>\n","      <th>cons.conf.idx</th>\n","      <th>euribor3m</th>\n","      <th>nr.employed</th>\n","      <th>job_blue-collar</th>\n","      <th>...</th>\n","      <th>month_may</th>\n","      <th>month_nov</th>\n","      <th>month_oct</th>\n","      <th>month_sep</th>\n","      <th>day_of_week_mon</th>\n","      <th>day_of_week_thu</th>\n","      <th>day_of_week_tue</th>\n","      <th>day_of_week_wed</th>\n","      <th>poutcome_nonexistent</th>\n","      <th>poutcome_success</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>22975</th>\n","      <td>56</td>\n","      <td>5</td>\n","      <td>999</td>\n","      <td>0</td>\n","      <td>4.8</td>\n","      <td>93.444</td>\n","      <td>14.7</td>\n","      <td>4.965</td>\n","      <td>5228.1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14746</th>\n","      <td>26</td>\n","      <td>1</td>\n","      <td>999</td>\n","      <td>0</td>\n","      <td>4.8</td>\n","      <td>93.918</td>\n","      <td>8.1</td>\n","      <td>4.957</td>\n","      <td>5228.1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12505</th>\n","      <td>38</td>\n","      <td>1</td>\n","      <td>999</td>\n","      <td>0</td>\n","      <td>4.8</td>\n","      <td>93.918</td>\n","      <td>8.1</td>\n","      <td>4.960</td>\n","      <td>5228.1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6801</th>\n","      <td>33</td>\n","      <td>2</td>\n","      <td>999</td>\n","      <td>0</td>\n","      <td>4.5</td>\n","      <td>93.994</td>\n","      <td>14.4</td>\n","      <td>4.857</td>\n","      <td>5191.0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18389</th>\n","      <td>29</td>\n","      <td>3</td>\n","      <td>999</td>\n","      <td>0</td>\n","      <td>4.8</td>\n","      <td>93.918</td>\n","      <td>8.1</td>\n","      <td>4.968</td>\n","      <td>5228.1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 52 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5cb3020-9745-411c-ad50-9c6a7046ffb5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d5cb3020-9745-411c-ad50-9c6a7046ffb5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d5cb3020-9745-411c-ad50-9c6a7046ffb5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":127}]},{"cell_type":"markdown","source":["Now we have to train our model, and score it!\n","\n","Earlier, I had tried scaling it manually by checking to see what the minimum values were of the numerics. I thought using the minmaxScaler would help anyway... then I found that it might nix the experiment that the professor had come up with."],"metadata":{"id":"fvOKcvMBbMga"}},{"cell_type":"code","source":["# Importing the classifier\n","from sklearn.naive_bayes import CategoricalNB\n","\n","# from sklearn.preprocessing import MinMaxScaler #fixed import\n","\n","# scaler = MinMaxScaler()\n","# no_dummies_train = pd.DataFrame(scaler.fit_transform(no_dummies_train))\n","# no_dummies_test = pd.DataFrame(scaler.transform(no_dummies_test))\n","\n","# Creating the model instance\n","model = CategoricalNB()\n","\n","# Fitting the data\n","model.fit(no_dummies_train, y_train)\n","\n","# Creating two scores, one for train set, other for test set.\n","score1 = model.score(no_dummies_train, y_train)\n","score2 = model.score(no_dummies_test, y_test)"],"metadata":{"id":"boO6fCKWbfuX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see those scores!"],"metadata":{"id":"92ACW9Dou_dZ"}},{"cell_type":"code","source":["print(\"Categorical Naive Bayes after setting cons.conf.idx and emp.var.rate\",\n","      \"\\nto 0 or above, and using get_dummies to hotOneEncode categorical\",\n","      \"variables:\", \"\\n\")\n","print(\"Score for train set:\", score1, \"\\nScore for test set\" , score2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fk-l6ivTvBOM","executionInfo":{"status":"ok","timestamp":1655864537254,"user_tz":420,"elapsed":18,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"cae118f1-c94e-4ca1-fded-7e25a3a1e71c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Categorical Naive Bayes after setting cons.conf.idx and emp.var.rate \n","to 0 or above, and using get_dummies to hotOneEncode categorical variables: \n","\n","Score for train set: 0.8583182713318406 \n","Score for test set 0.85360524399126\n"]}]},{"cell_type":"markdown","source":["It turns out using the minmaxScaler scored more than .02 higher, but for now, let's move on with what the professor suggested. \n","\n","My Classifier is 85.36% accurate with my test set, and 85.83% accurate on the training set.\n","\n","The next thing we are going to do, is check to see how many unique values we have in age. We can use Pandas .value_counts() method for this"],"metadata":{"id":"zZgcyuC6wfxe"}},{"cell_type":"code","source":["print(raw_data['age'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iGBCZODCPwBI","executionInfo":{"status":"ok","timestamp":1655864537254,"user_tz":420,"elapsed":17,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"a3dc7c49-d0e1-4664-bb1b-d48f126c3a77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["31    1947\n","32    1846\n","33    1833\n","36    1780\n","35    1759\n","      ... \n","89       2\n","91       2\n","94       1\n","87       1\n","95       1\n","Name: age, Length: 78, dtype: int64\n"]}]},{"cell_type":"markdown","source":["There are 78 unique values in the age category, but as you can see, some values are more popular than others. The zero-frequency problem may skew some of the data, since some data may have sparse information. Combining into larger groups may help.\n","\n","We are going to split them into bins of 10. I'm going to do it manually using a for loop, then see if there is a way to do it with a built-in function in pandas."],"metadata":{"id":"14m02ixASyMe"}},{"cell_type":"code","source":["# age_decade = pd.DataFrame([], columns = ['age_decade'])\n","# for a in raw_data['age']:\n","#   if a < 11:\n","#     age_decade.append(pd.DataFrame([10], columns = ['age_decade']))\n","#   if a < 21:\n","#     age_decade.append(pd.DataFrame([20], columns = ['age_decade']))\n","#   if a < 31:\n","#     age_decade.append(pd.DataFrame([30], columns = ['age_decade']))\n","#   if a < 41:\n","#     age_decade.append(pd.DataFrame([40], columns = ['age_decade']))\n","#   if a < 51:\n","#     age_decade.append(pd.DataFrame([50], columns = ['age_decade']))\n","#   if a < 61:\n","#     age_decade.append(pd.DataFrame([60], columns = ['age_decade']))\n","#   if a < 71:\n","#     age_decade.append(pd.DataFrame([70], columns = ['age_decade']))\n","#   if a < 81:\n","#     age_decade.append(pd.DataFrame([80], columns = ['age_decade']))\n","#   if a < 91:\n","#     age_decade.append(pd.DataFrame([90], columns = ['age_decade']))\n","#   if a < 101:\n","#     age_decade.append(pd.DataFrame([100], columns = ['age_decade']))\n","#   if a >= 101:\n","#     age_decade.append(pd.DataFrame([101], columns = ['age_decade']))"],"metadata":{"id":"WltvgOrEUqyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Given a combination of list instantiation, and repeated pandas.DataFrame calls, this approach is horrifically inefficient. I was in it to feel like a true Software Engineer, but we all know, that in reality, this isn't an efficient way. We are going to try using cut from the pandas package instead.\n","\n","I tested and tried like a good Data Scientist 😎."],"metadata":{"id":"KD0c1tg-W9AB"}},{"cell_type":"code","source":["# Group by decade, 10 being for anyone in their first decade, 20 for anyone in \n","# their second, etc.\n","raw_data['age_decade'] = pd.cut(raw_data['age'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n","                                labels = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n","print(raw_data['age_decade'])\n","\n","# Double checking the value_counts changed as intended\n","print(raw_data['age_decade'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojny6zGTXNeh","executionInfo":{"status":"ok","timestamp":1655864537255,"user_tz":420,"elapsed":9,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"3f5a27da-26ca-414c-8020-94d5906fbce1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0        60\n","1        60\n","2        40\n","3        40\n","4        60\n","         ..\n","41183    80\n","41184    50\n","41185    60\n","41186    50\n","41187    80\n","Name: age_decade, Length: 41188, dtype: category\n","Categories (10, int64): [10 < 20 < 30 < 40 ... 70 < 80 < 90 < 100]\n","40     16385\n","50     10240\n","30      7243\n","60      6270\n","70       488\n","80       303\n","20       140\n","90       109\n","100       10\n","10         0\n","Name: age_decade, dtype: int64\n"]}]},{"cell_type":"markdown","source":["So far so good, now let's try throwing those other values in the model. I just copy pasted from above."],"metadata":{"id":"eM1rgPdUedpL"}},{"cell_type":"code","source":["\n","# Made a new instantiation, since this data doesn't take up too much disc/RAM.\n","raw_data2 = raw_data.copy()\n","\n","# Drop the age variable.\n","raw_data2 = raw_data2.drop('age', axis = 1)\n","\n","# Stratify the data AGAIN, and retitled the data to protect from confusion\n","train_data2, test_data2, y_train2, y_test2 = train_test_split( raw_data2 , y_val,\n","                                  test_size = .1, random_state=(2021-10-25))\n","\n","# Doing that get_dummies song and dance, again!\n","no_dummies_train2 = pd.get_dummies(train_data2, drop_first=True)\n","no_dummies_test2 = pd.get_dummies(test_data2, drop_first=True)\n","\n","# Creating a new NaiveBayes model\n","model2 = CategoricalNB()\n","model2.fit(no_dummies_train2, y_train2)\n","\n","# Score 'em again!\n","score3 = model2.score(no_dummies_train2, y_train2)\n","score4 = model2.score(no_dummies_test2, y_test2)\n","\n","print(\"Categorical Naive Bayes after setting cons.conf.idx and emp.var.rate\",\n","      \"\\nto 0 or above, and using get_dummies to hotOneEncode categorical\",\n","      \"variables,\\nand have changed integer age values to bins based on decade:\",\n","      \"\\n\")\n","print(\"Score on training data:\", score3, \"\\nScore on testing data:\" , score4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KlJbcPEVfPKp","executionInfo":{"status":"ok","timestamp":1655864537863,"user_tz":420,"elapsed":613,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"57d6b8a1-13df-4677-e6a6-3fe1e160af7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Categorical Naive Bayes after setting cons.conf.idx and emp.var.rate \n","to 0 or above, and using get_dummies to hotOneEncode categorical variables,\n","and have changed integer age values to bins based on decade: \n","\n","Score on training data: 0.858534085084572 \n","Score on testing data: 0.85360524399126\n"]}]},{"cell_type":"markdown","source":["It looks like the training set scored a little bit better, and the test set almost the exact same. I guess the change to the data meant nothing!\n","\n","Next, we are going to try a KNeighborsClassifier. Oh boy, I hope it's better!"],"metadata":{"id":"yp5YDv-7hdqS"}},{"cell_type":"code","source":["# Import KNeighborsClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# Create a KNeighborsClassifier instance\n","model3 = KNeighborsClassifier()\n","\n","# Fit the model, using our first DataSet (age feature unaltered)\n","model3.fit(no_dummies_train, y_train)\n","\n","# Score em' again\n","score5 = model3.score(no_dummies_train, y_train)\n","score6 = model3.score(no_dummies_test, y_test)"],"metadata":{"id":"BbASX96UjQXG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's check the score!"],"metadata":{"id":"_Ko3b0Xyj2sg"}},{"cell_type":"code","source":["print(\"K Neighbors Classifier after setting cons.conf.idx and emp.var.rate\",\n","      \"\\nto 0 or above, and using get_dummies to hotOneEncode categoricals.\\n\",)\n","\n","print(\"Score 1:\", score5, \"\\nScore 2:\" , score6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3xn4f3tJj1f6","executionInfo":{"status":"ok","timestamp":1655864574071,"user_tz":420,"elapsed":39,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"b0f7fe4a-1bb4-4347-8e84-b6d78ff3dcb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["K Neighbors Classifier after setting cons.conf.idx and emp.var.rate \n","to 0 or above, and using get_dummies to hotOneEncode categoricals.\n","\n","Score 1: 0.9136744989074429 \n","Score 2: 0.8846807477543093\n"]}]},{"cell_type":"markdown","source":["Yeah. The KNN classifier seems to be doing better without much touching. Let's see what the probability of having a certain y value is, and judge this against the score."],"metadata":{"id":"ek8VcEIvkOyO"}},{"cell_type":"code","source":["print(y_test.value_counts()['no']/ (y_test.value_counts()['yes']+ y_test.value_counts()['no']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjOMqSCaldAb","executionInfo":{"status":"ok","timestamp":1655864574072,"user_tz":420,"elapsed":30,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"70f3281a-0949-432c-fa98-15fac860fb53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8851663025006069\n"]}]},{"cell_type":"markdown","source":["Houston, we have a problem. There might be bias for classifying everything as no. Let's try making a confusion matrix! The roc auc score should also help determine how precise our classifier is."],"metadata":{"id":"NgC9pWJ0mQmE"}},{"cell_type":"code","source":["# Importing the confusion_matrix metric, and the roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_auc_score\n","\n","# Creating prediction values by using the KNeighborsClassifier with\n","# original data.\n","model3_ytest = model3.predict(no_dummies_test)\n","\n","# My first confusion matrix!\n","ze_matrix = confusion_matrix(y_test, model3_ytest)\n","tn, fp, fn, tp = ze_matrix.ravel()\n","print(\"True negative: \", tn, \"\\nFalse positive:\", fp, \"\\nFalse negative\",\n","      fn, \"\\nTrue positive:\", tp)\n","print(ze_matrix)\n","\n","# Tried calculating AUC ROC manually, then realized it requires calculus!!\n","# true_positive_rate = tp / (tp + fn)\n","# false_positive_rate = fp / (tn + fn)\n","\n","# roc_auc_score uses binary np.array's for input, so need to create\n","# np.arrays to substitute. Will first create equal-size arrays full of\n","# false values.\n","the_answer3 = np.zeros_like(model3_ytest, dtype = np.dtype(bool))\n","the_original = np.zeros_like(y_test, dtype = np.dtype(bool))\n","\n","# Creating analogous numpy arrays to the y-output arrays. They use 0's and\n","# 1's rather than yes's and no's, or True's and False's.\n","for i in range(len(model3_ytest)):\n","  if model3_ytest[i] == 'yes':\n","    the_answer3[i] = True\n","  else:\n","    the_answer3[i] = False\n","  if y_test.iloc[i] == 'yes':\n","    the_original[i] = True\n","  else:\n","    the_original[i] = False\n","\n","score7 = roc_auc_score(the_original, the_answer3)\n","print(\"The ROC AUC score of the K Neighbors Classifier is:\", score7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DR2Fi4dnXCH","executionInfo":{"status":"ok","timestamp":1655864577851,"user_tz":420,"elapsed":3801,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"3eafd43d-81b8-47de-e018-03279ffa9686"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True negative:  3517 \n","False positive: 129 \n","False negative 346 \n","True positive: 127\n","[[3517  129]\n"," [ 346  127]]\n","The ROC AUC score of the K Neighbors Classifier is: 0.6165588516013958\n"]}]},{"cell_type":"markdown","source":["Now, we are going to check the roc auc score for our other two classifiers"],"metadata":{"id":"ws_kZEK465pG"}},{"cell_type":"code","source":["# For the Naive Bayes\n","\n","# Create the prediction array\n","model_ytest = model.predict(no_dummies_test)\n","\n","# Create another confusion matrix\n","ze_matrix2 = confusion_matrix(y_test, model_ytest)\n","\n","# Print them nicely!\n","tn2, fp2, fn2, tp2 = ze_matrix.ravel()\n","print(\"True negative: \", tn2, \"\\nFalse positive:\", fp2, \"\\nFalse negative\",\n","      fn2, \"\\nTrue positive:\", tp2)\n","print(ze_matrix2)\n","\n","# Creating the binary array needed to find the roc_auc_score\n","the_answer1 = np.zeros_like(model_ytest, dtype = np.dtype(bool))\n","\n","# Creating analogous numpy arrays to the y-output array.\n","for i in range(len(model_ytest)):\n","  if model_ytest[i] == 'yes':\n","    the_answer1[i] = True\n","  else:\n","    the_answer1[i] = False\n","\n","# Finding the roc_auc_score of the first classifier of this project\n","score8 = roc_auc_score(the_original, the_answer1)\n","print(\"The ROC AUC score of the first Naive Bayes model is:\", score8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NIi5kqlF6-i7","executionInfo":{"status":"ok","timestamp":1655864577852,"user_tz":420,"elapsed":51,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"054f39ef-4b9d-4705-d738-6d1d451d8072"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True negative:  3517 \n","False positive: 129 \n","False negative 346 \n","True positive: 127\n","[[3262  384]\n"," [ 219  254]]\n","The ROC AUC score of the first Naive Bayes model is: 0.7158384931095387\n"]}]},{"cell_type":"markdown","source":["Now we need to do the same thing, but this time, for our KNeighborsClassifier with age bins."],"metadata":{"id":"hRfREz1D7KV4"}},{"cell_type":"code","source":["# For the Naive_Bayes with age_bins. \n","# Quick note: we are using the SECOND Dataset\n","\n","# Create the predictions array\n","model2_ytest = model2.predict(no_dummies_test2)\n","\n","# Create another confusion_matrix\n","ze_matrix3 = confusion_matrix(y_test2, model2_ytest)\n","\n","# Print the confusion matrix nicely\n","tn3, fp3, fn3, tp3 = ze_matrix3.ravel()\n","print(\"True negative: \", tn3, \"\\nFalse positive:\", fp3, \"\\nFalse negative\",\n","      fn3, \"\\nTrue positive:\", tp3)\n","print(ze_matrix3)\n","\n","# Creating the binary array needed to find the roc_auc_score\n","the_answer2 = np.zeros_like(model2_ytest, dtype = np.dtype(bool))\n","the_original2 = np.zeros_like(y_test2, dtype = np.dtype(bool))\n","\n","# Creating analogous numpy arrays to the y-output arrays.\n","for i in range(len(model2_ytest)):\n","  if model2_ytest[i] == 'yes':\n","    the_answer2[i] = True\n","  else:\n","    the_answer2[i] = False\n","  if y_test2.iloc[i] == 'yes':\n","    the_original2[i] = True\n","  else:\n","    the_original2[i] = False\n","\n","# Finding the roc_auc_score of the second classifier of this project\n","score9 = roc_auc_score(the_original2, the_answer2)\n","print(\"The ROC AUC score of the first Naive Bayes model is:\",score9, \n","      \"\\nNote: This data's been altered to use decade bins rather than age.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUqiBkCG8kj5","executionInfo":{"status":"ok","timestamp":1655864577853,"user_tz":420,"elapsed":45,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"98ca1c42-8c8f-4861-8012-debf3b47e892"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True negative:  3261 \n","False positive: 385 \n","False negative 218 \n","True positive: 255\n","[[3261  385]\n"," [ 218  255]]\n","The ROC AUC score of the first Naive Bayes model is: 0.7167584389739284 \n","Note: This data's been altered to use decade bins rather than age.\n"]}]},{"cell_type":"markdown","source":["It's hard for me to know what a good roc auc score is, but both classifiers performed about the same level. I could assume 70% ROC AUC score isn't good for classifiers.\n","\n","But the point made that the KNeighbors classifiers has bias to say negative all the time is true, and you can see that it's precision score is 10 points (10%) beneath the other two.\n","\n","We are going to try to deal with this by oversampling to see if we can improve our precision. We have around ~12% yes's, let's try to move this closer to 50%, and then see if our models are any more accurate."],"metadata":{"id":"3TrtQ7iW_D3W"}},{"cell_type":"code","source":["raw_data_yes = raw_data_copy[raw_data_copy['y'] == 'yes']\n","raw_data_no = raw_data_copy[raw_data_copy['y'] == 'no']\n","print(raw_data_yes.shape, raw_data_no.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"db6q4WkpFgdg","executionInfo":{"status":"ok","timestamp":1655864577854,"user_tz":420,"elapsed":34,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"90faedfb-aaf6-40af-d85b-e49f044bc0b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(4640, 20) (36548, 20)\n"]}]},{"cell_type":"markdown","source":["I'm going to do random sampling with replacement for the yes values. I will sample the same amount that there are in no's."],"metadata":{"id":"tEwdbA6IkmT6"}},{"cell_type":"code","source":["new_raw_data_yes = raw_data_yes.sample(n = 36548, replace = True, random_state = (2021-10-25))\n","raw_data_overSample= new_raw_data_yes.append(raw_data_no)\n","print(raw_data_overSample.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCAlpIRskl6k","executionInfo":{"status":"ok","timestamp":1655864577855,"user_tz":420,"elapsed":28,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"8998cae8-44a3-4dc0-9ad7-f0966570477f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(73096, 20)\n"]}]},{"cell_type":"markdown","source":["Now, we have equal amount of yes's and no's in our data. Now, we need to reclean our data, create the models, score them,  generate the confusion matrices and take the roc auc score."],"metadata":{"id":"fQ6kkb-TmNT7"}},{"cell_type":"code","source":["y_val = raw_data_overSample['y']\n","raw_data = raw_data_overSample.drop('y', axis = 1)\n","raw_data = pd.get_dummies(raw_data, drop_first=True)\n","train_data, test_data, y_train, y_test = train_test_split( raw_data ,\n","                              y_val, test_size = .1, random_state=(2021-10-25))\n","\n","# no_dummies_train = pd.get_dummies(train_data, drop_first=True)\n","# no_dummies_test = pd.get_dummies(test_data, drop_first=True)\n","\n","model3 = KNeighborsClassifier()\n","model3.fit(train_data, y_train)\n","score10 = model3.score(train_data, y_train)\n","score11 = model3.score(test_data, y_test)\n","\n","\n","print(\"Score 1:\", score10, \"\\nScore 2:\" , score11)\n","\n","\n","\n","raw_data2 = raw_data_overSample.copy()\n","raw_data2['age_decade'] = pd.cut(raw_data2['age'], [0, 10, 20, 30, 40, 50, 60, \n","                                                    70, 80, 90, 100], labels = \n","                                [10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n","raw_data2 = raw_data2.drop('age', axis = 1)\n","raw_data2 = pd.get_dummies( raw_data2, drop_first = True)\n","train_data2, test_data2, y_train2, y_test2 = train_test_split( raw_data2 , y_val,\n","                                  test_size = .1, random_state=(2021-10-25))\n","\n","model2 = CategoricalNB()\n","model2.fit(train_data2, y_train2)\n","score12 = model2.score(train_data2, y_train2)\n","score13= model2.score(test_data2, y_test2)\n","\n","print(\"Score 1:\", score12, \"\\nScore 2:\" , score13)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7PrtUdkloYIL","executionInfo":{"status":"ok","timestamp":1655864678663,"user_tz":420,"elapsed":100830,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"1dccfb92-7bd1-4823-a93d-b966b245e3c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Score 1: 0.9151947222813365 \n","Score 2: 0.878796169630643\n","Score 1: 0.9872769282218101 \n","Score 2: 0.9886456908344733\n"]}]},{"cell_type":"markdown","source":["Not sure what the information is worth yet, but the raw scores are significantly higher than before, especialy with Naive Bayes."],"metadata":{"id":"jHmyB16Ftlus"}},{"cell_type":"code","source":["model_ytest = model2.predict(test_data2)\n","# For the Naive Bayes\n","ze_matrix2 = confusion_matrix(y_test2, model_ytest)\n","tn2, fp2, fn2, tp2 = ze_matrix2.ravel()\n","print(\"True negative: \", tn2, \"\\nFalse positive:\", fp2, \"\\nFalse negative\",\n","      fn2, \"\\nTrue positive:\", tp2)\n","print(ze_matrix2)\n","\n","\n","the_answer1 = np.zeros_like(model_ytest, dtype = np.dtype(bool))\n","the_original2 = np.zeros_like(y_test2, dtype = np.dtype(bool))\n","\n","# Creating analogous numpy arrays to the y-output arrays. They use 0's and\n","# 1's rather than yes's and no's.\n","for i in range(len(model_ytest)):\n","  if model_ytest[i] == 'yes':\n","    the_answer1[i] = True\n","  else:\n","    the_answer1[i] = False\n","  if y_test2.iloc[i] == 'yes':\n","    the_original2[i] = True\n","  else:\n","    the_original2[i] = False\n","\n","score14 = roc_auc_score(the_original2, the_answer1)\n","print(score14)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655864678665,"user_tz":420,"elapsed":20,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"c5cbaf50-63d5-4300-f38c-91c3023ad805","id":"OyqmLyKkt_Y4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True negative:  3574 \n","False positive: 83 \n","False negative 0 \n","True positive: 3653\n","[[3574   83]\n"," [   0 3653]]\n","0.9886519004648618\n"]}]},{"cell_type":"markdown","source":["That's a lot better"],"metadata":{"id":"J9H_o3oOwTBs"}},{"cell_type":"code","source":["model2_ytest = model3.predict(test_data)\n","# For the KNN\n","ze_matrix3 = confusion_matrix(y_test, model2_ytest)\n","tn3, fp3, fn3, tp3 = ze_matrix3.ravel()\n","print(\"True negative: \", tn3, \"\\nFalse positive:\", fp3, \"\\nFalse negative\",\n","      fn3, \"\\nTrue positive:\", tp3)\n","print(ze_matrix3)\n","\n","\n","the_answer2 = np.zeros_like(model2_ytest, dtype = np.dtype(bool))\n","the_original1 = np.zeros_like(y_test, dtype = np.dtype(bool))\n","\n","# Creating analogous numpy arrays to the y-output arrays. They use 0's and\n","# 1's rather than yes's and no's.\n","for i in range(len(model2_ytest)):\n","  if model2_ytest[i] == 'yes':\n","    the_answer2[i] = True\n","  else:\n","    the_answer2[i] = False\n","  if y_test.iloc[i] == 'yes':\n","    the_original1[i] = True\n","  else:\n","    the_original1[i] = False\n","\n","score9 = roc_auc_score(the_original1, the_answer2)\n","print(score9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyJXsIuewYOU","executionInfo":{"status":"ok","timestamp":1655864688929,"user_tz":420,"elapsed":10277,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"17952882-a927-45f0-d1f0-875736d62eb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True negative:  2820 \n","False positive: 837 \n","False negative 49 \n","True positive: 3604\n","[[2820  837]\n"," [  49 3604]]\n","0.8788551196977683\n"]}]},{"cell_type":"markdown","source":["Based on the roc auc score, we can conclude that the naive bayes is significantly more accurate. Balancing the data made a huge difference!\n","\n","I've been asked to try making a Gaussian Bayes Classifier using input variables described as “social and economic context attributes” in bank-additional-names.txt."],"metadata":{"id":"wB6dzc4SxQm-"}},{"cell_type":"code","source":["# Importing Naive Bayes Gaussian\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Creating new data set with columns/features specified above in-tact\n","raw_data = raw_data_copy[['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', \n","                          'euribor3m', 'nr.employed', 'y']]\n","\n","# Stratifying the y (dependant) variable\n","y_val = raw_data['y']\n","\n","# Dropping the dependant variable from the rest of the data\n","raw_data = raw_data.drop('y', axis = 1)\n","\n","# Separating the data between train data and test data\n","train_data, test_data, y_train, y_test = train_test_split( raw_data ,\n","                              y_val, test_size = .1, random_state=(2021-10-25))\n","\n","# Model instantiation\n","model4 = GaussianNB()\n","\n","# Fitting the model\n","model4.fit(train_data, y_train)\n","\n","# Scoring the model based on training data and on test data.\n","score15 = model4.score(train_data, y_train)\n","score16 = model4.score(test_data, y_test)\n","\n","# Displaying the scores\n","print(\"Guassian Naive Bayes classifier using only the variables specified as\",\n","      \"\\nsocial and economic context attributes.\\n\",)\n","\n","print(\"Score based on training data:\", score15, \n","      \"\\nScore based on test data:\" , score16)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYCaMp1VGFDG","executionInfo":{"status":"ok","timestamp":1655864689138,"user_tz":420,"elapsed":231,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"24738048-f961-4d1b-81d8-1d873af34a29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Guassian Naive Bayes classifier using only the variables specified as \n","social and economic context attributes.\n","\n","Score based on training data: 0.7199816558310178 \n","Score based on test data: 0.7193493566399611\n"]}]},{"cell_type":"markdown","source":["So far so good, let's check the AUC and confusion matrix."],"metadata":{"id":"s85OsZ3EI96A"}},{"cell_type":"code","source":["# Creating prediction values by using the GaussianNB classifier with\n","# data specified as social and economic context attributes.\n","model4_ytest = model4.predict(test_data)\n","\n","# Another confusion matrix!\n","ze_matrix = confusion_matrix(y_test, model4_ytest)\n","tn, fp, fn, tp = ze_matrix.ravel()\n","print(\"True negative: \", tn, \"\\nFalse positive:\", fp, \"\\nFalse negative\",\n","      fn, \"\\nTrue positive:\", tp)\n","print(ze_matrix)\n","\n","# To create equal-size arrays full of false values.\n","the_answer4 = np.zeros_like(model4_ytest, dtype = np.dtype(bool))\n","the_original = np.zeros_like(y_test, dtype = np.dtype(bool))\n","\n","# Creating analogous numpy arrays to the y-output arrays. They use 0's and\n","# 1's rather than yes's and no's, or True's and False's.\n","for i in range(len(model4_ytest)):\n","  if model4_ytest[i] == 'yes':\n","    the_answer4[i] = True\n","  else:\n","    the_answer4[i] = False\n","  if y_test.iloc[i] == 'yes':\n","    the_original[i] = True\n","  else:\n","    the_original[i] = False\n","\n","score17 = roc_auc_score(the_original, the_answer4)\n","print(\"The ROC AUC score of the Gaussian Naive Bayes is:\", score17)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nrobkJzsI9ny","executionInfo":{"status":"ok","timestamp":1655864689140,"user_tz":420,"elapsed":9,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"5dcdafae-b152-45fe-8c6a-75366d6e7b78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True negative:  2634 \n","False positive: 1012 \n","False negative 144 \n","True positive: 329\n","[[2634 1012]\n"," [ 144  329]]\n","The ROC AUC score of the Gaussian Naive Bayes is: 0.7089978997517046\n"]}]},{"cell_type":"markdown","source":["The results of the Gaussian Naive Bayes is comparable with the score of the Categorical Bayes when using the original dataset. The AUC score is .7089978997517046, a little bit less than .01 less than the naive bayes models.\n","\n","Now, we are going to try oversampling again for this model, and see how much better our Guassian Naive Bayes classifier gets."],"metadata":{"id":"q-hWvYnKKbpt"}},{"cell_type":"code","source":["# Using our oversampled data_set from above, and selecting the categories that\n","# can be described as “social and economic context attributes” in \n","# bank-additional-names.txt\n","raw_data_overSample = raw_data_overSample[['emp.var.rate', 'cons.price.idx', \n","                                          'cons.conf.idx', \n","                                          'euribor3m', 'nr.employed', 'y']]\n","\n","# Drop the y-value and save it as a separate series\n","y_val = raw_data_overSample['y']\n","raw_data = raw_data_overSample.drop('y', axis = 1)\n","\n","# Stratify the data as normal.\n","train_data, test_data, y_train, y_test = train_test_split( raw_data ,\n","                              y_val, test_size = .1, random_state=(2021-10-25))\n","\n","# Model instantiation\n","model5 = GaussianNB()\n","\n","# Fitting the model\n","model5.fit(train_data, y_train)\n","\n","# Scoring the model based on training data and on test data.\n","score18 = model5.score(train_data, y_train)\n","score19 = model5.score(test_data, y_test)\n","\n","# Displaying the scores\n","print(\"Guassian Naive Bayes classifier using only the variables specified as\",\n","      \"\\nsocial and economic context attributes, and using oversampling.\\n\",)\n","\n","print(\"Score based on training data:\", score18, \n","      \"\\nScore based on test data:\" , score19)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WEZAZCF4LEtS","executionInfo":{"status":"ok","timestamp":1655864689641,"user_tz":420,"elapsed":506,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"bcf142d9-ad64-46ee-b247-560c010d5d6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Guassian Naive Bayes classifier using only the variables specified as \n","social and economic context attributes, and using oversampling.\n","\n","Score based on training data: 0.7174930836348159 \n","Score based on test data: 0.7108071135430917\n"]}]},{"cell_type":"markdown","source":["Ok, well I guess that our data is isn't very good."],"metadata":{"id":"AqHAt6DRM_6h"}},{"cell_type":"code","source":["# Creating prediction values by using the GaussianNB classifier with\n","# data specified as social and economic context attributes.\n","model5_ytest = model5.predict(test_data)\n","\n","# Another confusion matrix!\n","ze_matrix = confusion_matrix(y_test, model5_ytest)\n","tn, fp, fn, tp = ze_matrix.ravel()\n","print(\"True negative: \", tn, \"\\nFalse positive:\", fp, \"\\nFalse negative\",\n","      fn, \"\\nTrue positive:\", tp)\n","print(ze_matrix)\n","\n","# To create equal-size arrays full of false values.\n","the_answer5 = np.zeros_like(model5_ytest, dtype = np.dtype(bool))\n","the_original = np.zeros_like(y_test, dtype = np.dtype(bool))\n","\n","# Creating analogous numpy arrays to the y-output arrays. They use 0's and\n","# 1's rather than yes's and no's, or True's and False's.\n","for i in range(len(model5_ytest)):\n","  if model5_ytest[i] == 'yes':\n","    the_answer5[i] = True\n","  else:\n","    the_answer5[i] = False\n","  if y_test.iloc[i] == 'yes':\n","    the_original[i] = True\n","  else:\n","    the_original[i] = False\n","\n","score20 = roc_auc_score(the_original, the_answer5)\n","print(\"The ROC AUC score of the Gaussian Naive Bayes is:\", score20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UQ0Hx9l4LA-M","executionInfo":{"status":"ok","timestamp":1655864689642,"user_tz":420,"elapsed":8,"user":{"displayName":"Jusin002","userId":"12085437777671714687"}},"outputId":"1b39c709-1699-4a3e-f578-5c438ed56f98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True negative:  2619 \n","False positive: 1038 \n","False negative 1076 \n","True positive: 2577\n","[[2619 1038]\n"," [1076 2577]]\n","The ROC AUC score of the Gaussian Naive Bayes is: 0.7108041824322306\n"]}]},{"cell_type":"markdown","source":["Now we can see, that the AUC score did not improve, if only neglibly. The lesson of the story is that if the data is good, picking the model that fits the data can make a big difference in how well your model works.\n","\n","The results did not significantly improve with oversampling, using the Gaussian Naive Bayes."],"metadata":{"id":"uB50MNS9NWN6"}}]}